#!/usr/bin/env python3
"""
=========================
s1f - Split One File
=========================

SYNOPSIS
========
Splits a single input file (created by the m1f.py tool) back into
multiple files, recreating the original directory structure within a specified
destination directory.

DESCRIPTION
===========
This script is the counterpart to m1f.py (Make One File). It reads a specially formatted
text file where multiple original files have been concatenated, each preceded by
a metadata separator. It parses these separators to determine the original
relative file paths and extracts the content for each file, saving them to the
destination directory.

It is designed to work with files generated by `m1f.py` and supports
all of its separator styles: 'Standard', 'Detailed', 'Markdown' and 'MachineReadable'.

KEY FEATURES
============
- Parses input files with all supported separator styles: 'Standard', 'Detailed', 'Markdown', and 'MachineReadable'.
- Properly extracts original file paths from all separator formats, ensuring consistent behavior.
- Recreates the original directory structure based on paths found in separators.
- Handles both LF and CRLF line endings in separator patterns and content processing.
- Respects original file encoding when available and requested with --respect-encoding.
- Supports explicit output encoding specification with --target-encoding.
- Verifies file integrity using SHA256 checksums when available.
- Option to force overwrite of existing files in the destination.
- Verbose mode for detailed operational logging.
- Secure path handling to prevent writing outside the destination directory.

REQUIREMENTS
============
- Python 3.9+ (due to `pathlib` usage and f-strings)
- Standard Python libraries only (argparse, logging, os, pathlib, re, sys).

USAGE
=====
Basic command:
  python tools/s1f.py --input-file /path/to/combined_output.txt --destination-directory /path/to/output_folder

With force overwrite and verbose output:
  python tools/s1f.py -i combined.txt -d ./extracted_files -f -v

With original encoding preservation:
  python tools/s1f.py -i combined.txt -d ./extracted_files --respect-encoding

With explicit encoding specification:
  python tools/s1f.py -i combined.txt -d ./extracted_files --target-encoding utf-8

For all options, run:
  python tools/s1f.py --help

NOTES
=====
- Encoding: The script reads the input file as UTF-8 (matching `m1f.py`'s output).
  By default, it writes extracted files as UTF-8. There are two ways to control output encoding:
  1. With `--respect-encoding`, it will attempt to use the original encoding of each file
     as recorded in the metadata.
  2. With `--target-encoding`, you can explicitly specify the encoding to use for all extracted files,
     which overrides any original encoding information (e.g., `--target-encoding utf-8`).
  When both options are provided, `--target-encoding` takes precedence.
- Line Endings: The script correctly identifies separators and content
  regardless of LF or CRLF line endings used in the input file. The content of
  the extracted files will retain the line endings as they were in the combined file.
- Separator Integrity: The script reliably handles all separator styles from m1f.py:
  * Standard: Simple format with file path and optional checksum
  * Detailed: Extended format with additional metadata
  * Markdown: GitHub-friendly format with code blocks
  * MachineReadable: JSON metadata format with precise boundaries
- Path Preservation: All separator styles correctly extract and preserve the original file paths,
  ensuring the reconstructed directory structure matches the original.

AUTHOR
======
Franz und Franz (https://franz.agency)
Project: https://m1f.dev

VERSION
=======
2.0.0
"""

import argparse
import hashlib
import json
import logging
import os
import re
import sys
from pathlib import Path, PureWindowsPath

from path_utils import convert_to_posix_path
from datetime import datetime, timezone

# --- Logger Setup ---
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO, format="%(levelname)-8s: %(message)s")

# --- Global Definitions ---
LF = "\n"
CRLF = "\r\n"

# Separator Regex Patterns
# These regexes are designed to match the beginning of a separator block
# and capture the relative file path. The order in `separator_patterns`
# in `parse_combined_file` is important (most specific first).
#
# Each regex aims to capture:
# Group 1: The entire separator header (used to determine header length).
# Group 2: The relative file path.

# New regex pattern for the PYMK1F format with UUID-based separators
RE_PYMK1F_SEP = re.compile(
    r"--- PYMK1F_BEGIN_FILE_METADATA_BLOCK_([a-f0-9-]+) ---\r?\n"  # Metadata start with UUID capture (Group 1)
    r"METADATA_JSON:\r?\n"  # Metadata indicator line
    r"(\{(?:.|\s)*?\})\r?\n"  # JSON metadata capture (Group 2) - better handling of multiline JSON
    r"--- PYMK1F_END_FILE_METADATA_BLOCK_\1 ---\r?\n"  # Metadata end with same UUID
    r"--- PYMK1F_BEGIN_FILE_CONTENT_BLOCK_\1 ---",  # Content start with same UUID - removed trailing \r?\n
    re.MULTILINE | re.DOTALL,  # Added DOTALL for better handling of newlines in JSON
)

# Pattern for the end of a content block
PYMK1F_END_MARKER_PATTERN = r"--- PYMK1F_END_FILE_CONTENT_BLOCK_([a-f0-9-]+) ---"

# Legacy format patterns (keeping for backward compatibility)
RE_MACHINE_SEP = re.compile(
    r"# PYM1F-BOUNDARY-99C5F740A78D4ABC82E3F9882D5A281E\r?\n"  # Start marker line
    r"# FILE: (.*?)\r?\n"  # File path line (Group 1: file path)
    r"# PYM1F-BOUNDARY-99C5F740A78D4ABC82E3F9882D5A281E\r?\n"  # Second boundary line
    r"# METADATA: (\{.*?\})\r?\n"  # JSON metadata line (Group 2: JSON string)
    r"# PYM1F-BOUNDARY-99C5F740A78D4ABC82E3F9882D5A281E\r?\n",  # Bottom separator line
    re.MULTILINE,
)
# Legacy format end marker patterns
MACHINE_END_MARKER_PATTERN = r"# PYM1F-BOUNDARY-99C5F740A78D4ABC82E3F9882D5A281E\r?\n# END FILE\r?\n# PYM1F-BOUNDARY-99C5F740A78D4ABC82E3F9882D5A281E"
MACHINE_END_MARKER = "# PYM1F-BOUNDARY-99C5F740A78D4ABC82E3F9882D5A281E\n# END FILE\n# PYM1F-BOUNDARY-99C5F740A78D4ABC82E3F9882D5A281E"

RE_DETAILED_SEP = re.compile(
    r"^(========================================================================================$\r?\n"
    r"== FILE: (.*?)$\r?\n"  # Group 2: Relative path
    r"== DATE: .*? \| SIZE: .*? \| TYPE: .*?$\r?\n"
    r"(?:== ENCODING: (.*?)(?:\s\(with conversion errors\))?$\r?\n)?"  # Group 3: Optional Encoding
    r"(?:== CHECKSUM_SHA256: ([0-9a-fA-F]{64})$\r?\n)?"  # Group 4: Optional Checksum
    r"========================================================================================$\r?\n?)",
    re.MULTILINE,
)

RE_STANDARD_SEP = re.compile(
    r"======= (.*?)(?:\s*\|\s*CHECKSUM_SHA256:\s*([0-9a-fA-F]{64}))?\s*======",  # Group 1: Path, Group 2: Optional Checksum
    re.MULTILINE,
)

RE_MARKDOWN_SEP = re.compile(
    r"^(## (.*?)$\r?\n"  # Group 2: Relative path
    # Non-capturing group for the whole metadata line, with optional encoding and checksum parts
    r"(?:\*\*Date Modified:\*\* .*? \| \*\*Size:\*\* .*? \| \*\*Type:\*\* .*?"
    r"(?:\s\|\s\*\*Encoding:\*\*\s(.*?)(?:\s\(with conversion errors\))?)?"  # Group 3: Optional Encoding
    r"(?:\s\|\s\*\*Checksum \(SHA256\):\*\*\s([0-9a-fA-F]{64}))?)$\r?\n\r?\n"  # Group 4: Optional Checksum
    r"```(?:.*?)$\r?\n)",
    re.MULTILINE,
)


# --- Helper Functions ---


def _configure_logging(verbose: bool):
    """Configures logging level based on verbosity."""
    if verbose:
        logging.getLogger().setLevel(logging.DEBUG)
        logger.debug("Verbose mode enabled.")


def _resolve_input_file(input_file_str: str) -> Path:
    """Resolves and validates the source input file path."""
    try:
        input_file = Path(input_file_str).resolve(strict=True)
        if not input_file.is_file():
            logger.error(f"Input path '{input_file}' is not a file.")
            sys.exit(1)
        return input_file
    except FileNotFoundError:
        logger.error(f"Input file '{input_file_str}' not found.")
        sys.exit(1)
    except Exception as e:
        logger.error(f"Error resolving input file '{input_file_str}': {e}")
        sys.exit(1)


def _prepare_destination_dir(dest_dir_str: str) -> Path:
    """
    Resolves the destination directory path and creates it if it doesn't exist.

    Args:
        dest_dir_str: The path to the destination directory as a string.

    Returns:
        Path: The resolved and created directory path.

    Exits:
        SystemExit: If the directory cannot be created or accessed.
    """
    try:
        dest_dir = Path(dest_dir_str).resolve()
        dest_dir.mkdir(parents=True, exist_ok=True)
        logger.debug(f"Using destination directory: {dest_dir}")
        return dest_dir
    except Exception as e:
        logger.error(
            f"Error: Could not create or access destination directory '{dest_dir_str}': {e}"
        )
        sys.exit(1)


def parse_combined_file(content: str) -> list[dict]:
    """
    Parses the combined file content and extracts individual file data.
    The function identifies file separators using a list of regex patterns,
    sorts them by their appearance in the file, and then extracts
    the content between each separator and the next (or to the end-of-file marker
    for MachineReadable style). The integrity of MachineReadable files is also
    verified using a checksum.

    Args:
        content: The entire string content of the combined input file.

    Returns:
        A list of dictionaries, where each dict contains:
        'path': relative_path_str
        'content': file_content_str
        'encoding': file_encoding_str (if available)
        For 'MachineReadable', it also includes:
        'modified': modification_date_str
        'type': file_extension_str
        'size_bytes': original_size_int
        'checksum_sha256': original_checksum_str
    """
    extracted_files = []

    # Order matters: More specific/distinct patterns should come first.
    # PYMK1F and MachineReadable are very distinct.
    separator_patterns = [
        {
            "id": "PYMK1F",
            "regex": RE_PYMK1F_SEP,
            "uuid_group": 1,  # UUID capture in the regex
            "json_group": 2,  # JSON metadata
        },
        {
            "id": "MachineReadable",
            "regex": RE_MACHINE_SEP,
            "path_group": 1,
            "json_group": 2,
        },
        {
            "id": "Markdown",
            "regex": RE_MARKDOWN_SEP,
            "path_group": 2,
            "encoding_group": 3,  # Added encoding group
            "checksum_group": 4,  # Updated group number
            "header_group": 1,
        },
        {
            "id": "Detailed",
            "regex": RE_DETAILED_SEP,
            "path_group": 2,
            "encoding_group": 3,  # Added encoding group
            "checksum_group": 4,  # Updated group number
            "header_group": 1,
        },
        {
            "id": "Standard",
            "regex": RE_STANDARD_SEP,
            "path_group": 1,  # Group 1 is the file path in RE_STANDARD_SEP
            "checksum_group": 2,  # Group 2 is the checksum in RE_STANDARD_SEP
            "header_group": 0,
        },
    ]

    matches = []
    for pattern_info in separator_patterns:
        for match in pattern_info["regex"].finditer(content):
            path_val = ""
            checksum_val = None  # Initialize checksum for all patterns
            modified_val = None  # Initialize modified timestamp
            encoding_val = None  # Initialize encoding value
            file_info_dict = {
                "id": pattern_info["id"],
                "match_obj": match,
                "start_index": match.start(),
            }

            # Calculate header length for different formats
            if pattern_info["id"] == "PYMK1F":
                # For PYMK1F format, the header is everything from the beginning of metadata marker to the end of content marker
                header_text = match.group(0)
                file_info_dict["header_len"] = len(header_text)
                # Store the UUID for later use in finding the closing separator
                file_info_dict["uuid"] = match.group(pattern_info["uuid_group"])
            elif pattern_info["id"] == "MachineReadable":
                # For MachineReadable format, we need to account for a blank line after the last marker
                header_text = match.group(0)
                file_info_dict["header_len"] = len(header_text)
                # Check if a blank line follows the header
                next_pos = match.end()
                if next_pos < len(content) and content[next_pos : next_pos + 2] in [
                    "\r\n",
                    "\n",
                ]:
                    file_info_dict["header_len"] += (
                        2 if content[next_pos : next_pos + 2] == "\r\n" else 1
                    )
            elif "header_group" in pattern_info:
                file_info_dict["header_len"] = len(
                    match.group(pattern_info["header_group"])
                )

            if pattern_info["id"] == "PYMK1F":
                try:
                    # Get metadata from JSON
                    json_str = match.group(pattern_info["json_group"])
                    meta = json.loads(json_str)

                    # Extract path from metadata
                    path_val = meta.get("original_filepath", "").strip()
                    path_val = convert_to_posix_path(path_val)

                    # Check if we have a valid path
                    if not path_val:
                        logger.warning(
                            f"PYMK1F block found at offset {match.start()} with missing or empty path in metadata"
                        )
                        continue

                    # Extract timestamp from metadata (new format uses timestamp_utc_iso)
                    modified_val = meta.get("timestamp_utc_iso")

                    # Extract encoding information if available
                    encoding_val = meta.get("encoding")
                    had_encoding_errors = meta.get("had_encoding_errors", False)
                    if had_encoding_errors and encoding_val:
                        encoding_val += " (with conversion errors)"

                    file_info_dict.update(
                        {
                            "path": path_val,
                            "modified": modified_val,
                            "type": meta.get("type"),
                            "size_bytes": meta.get("size_bytes"),
                            "checksum_sha256": meta.get("checksum_sha256"),
                            "encoding": encoding_val,  # Add encoding information
                        }
                    )
                    matches.append(file_info_dict)
                    continue
                except json.JSONDecodeError as e:
                    logger.warning(
                        f"PYMK1F block found at offset {match.start()} with invalid JSON: {json_str}. Error: {e}"
                    )
                    continue
                except Exception as e:
                    logger.warning(
                        f"Error processing PYMK1F block at offset {match.start()} with JSON '{json_str}': {e}"
                    )
                    continue
            elif pattern_info["id"] == "MachineReadable":
                try:
                    # Get the path directly from the regex match
                    path_val = match.group(pattern_info["path_group"]).strip()
                    path_val = convert_to_posix_path(path_val)

                    # Get metadata from JSON
                    json_str = match.group(pattern_info["json_group"])
                    meta = json.loads(json_str)

                    # Check if we have a valid path
                    if not path_val:
                        logger.warning(
                            f"MachineReadable block found at offset {match.start()} with missing or empty path"
                        )
                        continue

                    modified_val = meta.get("modified")  # Extract modified timestamp
                    encoding_val = meta.get("encoding")  # Extract encoding if available

                    file_info_dict.update(
                        {
                            "path": path_val,
                            "modified": modified_val,  # Store it
                            "type": meta.get("type"),
                            "size_bytes": meta.get("size_bytes"),
                            "checksum_sha256": meta.get("checksum_sha256"),
                            "encoding": encoding_val,  # Add encoding information
                        }
                    )
                    matches.append(file_info_dict)
                    continue
                except json.JSONDecodeError as e:
                    logger.warning(
                        f"MachineReadable block found at offset {match.start()} with invalid JSON: {json_str}. Error: {e}"
                    )
                    continue
                except Exception as e:
                    logger.warning(
                        f"Error processing MachineReadable block at offset {match.start()} with JSON '{json_str}': {e}"
                    )
                    continue
            else:
                path_val = match.group(pattern_info["path_group"]).strip()
                path_val = convert_to_posix_path(path_val)

                # Extract encoding if available
                if (
                    "encoding_group" in pattern_info
                    and pattern_info["encoding_group"] <= len(match.groups())
                    and match.group(pattern_info["encoding_group"]) is not None
                ):
                    encoding_val = match.group(pattern_info["encoding_group"])

                # Extract checksum if available
                if (
                    "checksum_group" in pattern_info
                    and pattern_info["checksum_group"] <= len(match.groups())
                    and match.group(pattern_info["checksum_group"]) is not None
                ):
                    checksum_val = match.group(pattern_info["checksum_group"])
                else:
                    checksum_val = None

                logger.debug(
                    f"Extracted path '{path_val}' with encoding: {encoding_val}, checksum: {checksum_val}"
                )

            file_info_dict.update(
                {
                    "path": path_val,
                    "checksum_sha256": checksum_val,
                    "modified": modified_val,  # Will be None for non-MachineReadable types
                    "encoding": encoding_val,  # Add encoding information
                }
            )
            matches.append(file_info_dict)

    # Sort matches by their start index in the original content to process files in order.
    matches.sort(key=lambda m: m["start_index"])

    if not matches:
        logger.warning("No recognizable file separators found in the input file.")
        return []

    for i, current_match_info in enumerate(matches):
        sep_id = current_match_info["id"]
        match_obj = current_match_info["match_obj"]
        relative_path = current_match_info["path"]

        original_checksum = current_match_info.get("checksum_sha256")
        original_size_bytes = current_match_info.get("size_bytes")
        original_modified = current_match_info.get("modified")
        original_encoding = current_match_info.get("encoding")

        content_start_pos = match_obj.end()

        # Determine the end of the current file's content
        next_separator_start_pos = len(
            content
        )  # Default: end of the whole content string

        if sep_id == "PYMK1F":
            # For PYMK1F format, find the corresponding closing marker using the UUID
            file_uuid = current_match_info.get("uuid")
            if file_uuid:
                end_marker_pattern = (
                    f"--- PYMK1F_END_FILE_CONTENT_BLOCK_{file_uuid} ---"
                )
                end_marker_pos = content.find(end_marker_pattern, content_start_pos)

                if end_marker_pos != -1:
                    # Found the end marker
                    next_separator_start_pos = end_marker_pos
                else:
                    # End marker with matching UUID not found
                    logger.warning(
                        f"PYMK1F file '{relative_path}' is missing its end marker with UUID {file_uuid}. Content might be incomplete or incorrect."
                    )
                    # Fallback: content runs to the start of the next found separator or EOF
                    if i + 1 < len(matches):
                        next_separator_start_pos = matches[i + 1]["start_index"]
            else:
                # If UUID is not available (shouldn't happen with proper regex)
                logger.warning(
                    f"PYMK1F block for '{relative_path}' has no UUID. Cannot find matching end marker."
                )
                # Fallback: content runs to the start of the next found separator or EOF
                if i + 1 < len(matches):
                    next_separator_start_pos = matches[i + 1]["start_index"]
        elif sep_id == "MachineReadable":
            # Find the end marker for this specific MachineReadable block using regex to handle line endings
            end_marker_re = re.compile(MACHINE_END_MARKER_PATTERN, re.MULTILINE)
            end_marker_search = end_marker_re.search(content, content_start_pos)
            if end_marker_search is not None:
                end_marker_pos = end_marker_search.start()
                # Check for CRLF before the marker (e.g., content\r\nMARKER)
                if (
                    end_marker_pos > 1
                    and content[end_marker_pos - len(CRLF) : end_marker_pos] == CRLF
                ):
                    next_separator_start_pos = end_marker_pos - len(
                        CRLF
                    )  # Exclude CRLF
                # Else, check for LF before the marker (e.g., content\nMARKER)
                elif (
                    end_marker_pos > 0
                    and content[end_marker_pos - len(LF) : end_marker_pos] == LF
                ):
                    next_separator_start_pos = end_marker_pos - len(LF)  # Exclude LF
                else:
                    # No expected newline (LF or CRLF) found before the end marker.
                    # This implies the content runs flush to the marker, or the file is structured unexpectedly.
                    # m1f.py is expected to always add a newline (os.linesep).
                    logger.warning(
                        f"MachineReadable file '{relative_path}' did not have an expected newline (LF or CRLF) "
                        f"before its end marker at offset {end_marker_pos}. "
                        f"The extracted content will go up to the marker. This might be incorrect if a separator was missing."
                    )
                    next_separator_start_pos = (
                        end_marker_pos  # Content up to the marker
                    )
            else:  # MACHINE_END_MARKER not found
                logger.warning(
                    f"MachineReadable file '{relative_path}' is missing its end marker (a pattern like {MACHINE_END_MARKER}). Content might be incomplete or incorrect."
                )
                # Fallback: content runs to the start of the next found separator or EOF.
                if i + 1 < len(matches):
                    next_separator_start_pos = matches[i + 1]["start_index"]
                # else: next_separator_start_pos remains len(content)
        elif i + 1 < len(
            matches
        ):  # For non-MachineReadable types, content ends before the next separator
            next_separator_start_pos = matches[i + 1]["start_index"]
        # else: for the last file (non-MachineReadable), next_separator_start_pos remains len(content)

        file_content_raw = content[content_start_pos:next_separator_start_pos]
        file_content = ""

        if sep_id in ["PYMK1F", "MachineReadable"]:
            file_content = file_content_raw
            # ---- START PRAGMATIC FIX FOR TRAILING \r ----
            if (
                original_size_bytes is not None and original_checksum is not None
            ):  # Only apply if we have original metadata
                try:
                    current_content_bytes = file_content.encode("utf-8")
                    current_size_bytes = len(current_content_bytes)
                    if (
                        current_size_bytes == original_size_bytes + 1
                        and file_content.endswith("\r")
                    ):
                        # Verify if stripping the 'r' would match the original checksum
                        # This avoids incorrectly stripping an 'r' that was part of legitimate content
                        # and coincidentally made the size +1.
                        potential_fixed_content_bytes = file_content[:-1].encode(
                            "utf-8"
                        )
                        potential_fixed_checksum = hashlib.sha256(
                            potential_fixed_content_bytes
                        ).hexdigest()

                        if (
                            potential_fixed_checksum == original_checksum
                            and len(potential_fixed_content_bytes)
                            == original_size_bytes
                        ):
                            logger.info(
                                f"Applying pragmatic fix for '{relative_path}': stripping trailing '\r'. "
                                f"Original size: {original_size_bytes}, current size before fix: {current_size_bytes}. "
                                f"Checksum matches after fix."
                            )
                            file_content = file_content[:-1]
                        elif (
                            current_size_bytes == original_size_bytes + 1
                        ):  # still off by 1 byte, but checksum didn't match
                            logger.warning(
                                f"Pragmatic fix condition (size +1 byte, ends with '\\r') met for '{relative_path}', "
                                f"but stripping '\\r' would NOT match the original checksum. "
                                f"Expected checksum: {original_checksum}, checksum after potential fix: {potential_fixed_checksum}. "
                                f"File will be left as is, but is likely corrupted or was altered from original."
                            )
                except Exception as e_fix:
                    logger.warning(
                        f"Error during pragmatic fix attempt for '{relative_path}': {e_fix}"
                    )
            # ---- END PRAGMATIC FIX ----
        elif sep_id == "Markdown":
            # m1f.py writes:
            #   NormalizedFileContent
            #   "```" (from get_closing_separator)
            #   chosen_linesep (after closing separator)
            #   chosen_linesep (inter-file, if applicable)
            # So, file_content_raw is NormalizedFileContent + "```" + linesep1 + [optional_linesep2]

            _processed_content = file_content_raw
            # Strip the optional inter-file newline separator (if this is not the last file segment).
            if i + 1 < len(matches):
                if _processed_content.endswith(CRLF):
                    _processed_content = _processed_content[: -len(CRLF)]
                elif _processed_content.endswith(LF):
                    _processed_content = _processed_content[: -len(LF)]

            # Strip the "```" + its own trailing newline (closing marker part).
            if _processed_content.endswith("```" + CRLF):
                file_content = _processed_content[: -(len("```" + CRLF))]
            elif _processed_content.endswith("```" + LF):
                file_content = _processed_content[: -(len("```" + LF))]
            else:
                logger.warning(
                    f"Markdown file '{relative_path}' closing sequence '```[newline]' "
                    f"not found as expected. Content might be incorrect."
                )
                file_content = file_content_raw  # Fallback

        else:  # Standard or Detailed
            # m1f.py writes:
            #   chosen_linesep (blank line after header)
            #   NormalizedFileContent
            #   chosen_linesep (inter-file, if applicable)
            # So, file_content_raw is: BlankLinesep_after_header + ActualFileContent + [optional_InterFileLinesep]

            _processed_content = file_content_raw

            # Strip the leading blank line (the first linesep written by m1f after the header for these styles).
            if _processed_content.startswith(CRLF):
                _processed_content = _processed_content[len(CRLF) :]
            elif _processed_content.startswith(LF):
                _processed_content = _processed_content[len(LF) :]

            # Strip the optional trailing inter-file newline separator (if this is not the last file segment).
            if i + 1 < len(matches):
                if _processed_content.endswith(CRLF):
                    _processed_content = _processed_content[: -len(CRLF)]
                elif _processed_content.endswith(LF):
                    _processed_content = _processed_content[: -len(LF)]

            file_content = _processed_content

        extracted_files.append(
            {
                "path": relative_path,
                "content": file_content,
                "checksum_sha256": original_checksum,  # Will be None for non-MachineReadable
                "size_bytes": original_size_bytes,  # Will be None for non-MachineReadable
                "modified": original_modified,  # Pass through the original modified timestamp
                "encoding": original_encoding,  # Pass through the original encoding information
            }
        )
        logger.debug(
            f"Identified file: '{relative_path}', type: {sep_id}, content length: {len(file_content)}"
        )

    return extracted_files


def _write_extracted_files(
    dest_dir_path: Path,
    extracted_files_data: list[dict],
    force_overwrite: bool,
    timestamp_mode: str,
    ignore_checksum: bool = False,
    respect_encoding: bool = False,
    target_encoding: str = None,
) -> tuple[int, int, int]:
    """
    Writes the extracted file data to the destination directory.

    Args:
        dest_dir_path: The root directory where files will be written.
        extracted_files_data: A list of dictionaries, each containing the
                              relative path and content for a file.
        force_overwrite: Boolean indicating whether to overwrite existing files
                         without prompting.
        timestamp_mode: How to handle file timestamps ('original' or 'current').
        ignore_checksum: If True, skip checksum verification for MachineReadable files.
        respect_encoding: If True, try to use the original file encoding when writing files.
                          This is ignored if target_encoding is specified.
        target_encoding: If provided, override all other encoding settings and use this encoding.
                         This has the highest priority for determining the output encoding.

    Returns:
        A tuple containing (files_created_count, files_overwritten_count, files_failed_count).

    Encoding Priority Rules:
    1. If target_encoding is specified, it is used for all files.
    2. If respect_encoding is True and file has encoding metadata, that encoding is used.
    3. Otherwise, UTF-8 is used as the default encoding.
    """
    files_created_count = 0
    files_overwritten_count = 0
    files_failed_count = 0

    logger.info(
        f"Writing {len(extracted_files_data)} extracted file(s) to '{dest_dir_path}'..."
    )
    for file_data in extracted_files_data:
        relative_path_str = file_data["path"]
        normalized = convert_to_posix_path(relative_path_str)
        file_content_to_write = file_data["content"]
        original_checksum = file_data.get("checksum_sha256")
        original_size_bytes = file_data.get("size_bytes")
        original_modified = file_data.get(
            "modified"
        )  # Get original modification timestamp
        original_encoding = file_data.get("encoding")  # Get original file encoding

        # Security check: ensure relative paths do not try to escape the destination directory.
        if ".." in Path(normalized).parts:
            logger.error(
                f"Skipping file '{relative_path_str}' due to invalid path components ('..')."
            )
            files_failed_count += 1
            continue

        current_output_path = dest_dir_path / normalized

        logger.debug(f"Preparing to write: {current_output_path}")

        try:
            # Ensure parent directory exists.
            current_output_path.parent.mkdir(parents=True, exist_ok=True)

            if current_output_path.exists() and not force_overwrite:
                try:
                    confirmation = input(
                        f"Output file '{current_output_path}' already exists. Overwrite? (y/N): "
                    )
                    if confirmation.lower() != "y":
                        logger.info(f"Skipping existing file '{current_output_path}'.")
                        continue
                except KeyboardInterrupt:  # Handle Ctrl+C during prompt
                    logger.info(
                        f"{(LF if os.name != 'nt' else CRLF)}Operation cancelled by user (Ctrl+C)."
                    )
                    sys.exit(0)  # Graceful exit as user initiated stop before action

            is_overwrite = current_output_path.exists()

            # Determine encoding to use
            output_encoding = "utf-8"  # Default encoding
            encoding_msg = ""

            # If target_encoding is specified, it takes precedence over all other options
            if target_encoding:
                output_encoding = target_encoding
                encoding_msg = (
                    f" using explicitly specified encoding: {target_encoding}"
                )
            # Otherwise, use respect_encoding logic if enabled
            elif respect_encoding and original_encoding:
                # Clean up the encoding string to remove error information
                clean_encoding = original_encoding.split(" (with conversion errors)")[
                    0
                ].strip()

                # Only use valid encodings supported by Python
                try:
                    # Test if this is a valid encoding by trying to encode a simple string
                    "test".encode(clean_encoding)
                    output_encoding = clean_encoding
                    encoding_msg = f" using original encoding: {clean_encoding}"
                except (LookupError, UnicodeError):
                    logger.warning(
                        f"Original encoding '{clean_encoding}' for file '{relative_path_str}' is not recognized. "
                        f"Falling back to UTF-8."
                    )

            # Write the file with the appropriate encoding
            try:
                # First try to encode the content with the target encoding
                encoded_content = file_content_to_write.encode(
                    output_encoding, errors="strict"
                )
                current_output_path.write_bytes(encoded_content)
                logger.debug(f"Wrote file: {current_output_path}{encoding_msg}")
            except UnicodeEncodeError:
                # If strict encoding fails, fall back to replace mode
                logger.warning(
                    f"Cannot strictly encode file '{relative_path_str}' with {output_encoding}. "
                    f"Falling back to replacement mode which may lose some characters."
                )
                current_output_path.write_text(
                    file_content_to_write, encoding=output_encoding, errors="replace"
                )
                logger.debug(
                    f"Wrote file (with character replacements): {current_output_path}"
                )

            if is_overwrite:
                files_overwritten_count += 1
                logger.debug(f"Overwrote file: {current_output_path}")
            else:
                files_created_count += 1
                logger.debug(f"Created file: {current_output_path}")

            # Set file modification time if requested and available
            if timestamp_mode == "original" and original_modified:
                try:
                    # Ensure the timestamp is offset-aware, assuming UTC if 'Z' is present.
                    # fromisoformat expects +HH:MM for timezone, so replace 'Z' if needed.
                    if original_modified.endswith("Z"):
                        dt_obj = datetime.fromisoformat(
                            original_modified.replace("Z", "+00:00")
                        )
                    else:
                        # If no 'Z' and no explicit offset, it might be naive or already have an offset.
                        # We'll try parsing directly. If it's naive, timestamp() might use local timezone.
                        # For m1f, 'Z' (UTC) is standard for MachineReadable.
                        dt_obj = datetime.fromisoformat(original_modified)

                    # Convert to POSIX timestamp (seconds since epoch, UTC)
                    mod_time = dt_obj.timestamp()
                    access_time = (
                        mod_time  # Set access time to the same as modification time
                    )
                    os.utime(current_output_path, (access_time, mod_time))
                    logger.debug(
                        f"Set original modification time for '{current_output_path}' to {original_modified}"
                    )
                except ValueError:
                    logger.warning(
                        f"Could not parse original modification timestamp '{original_modified}' for '{current_output_path}'. Using current timestamp."
                    )
                except Exception as e:
                    logger.warning(
                        f"Could not set original modification time for '{current_output_path}' using timestamp '{original_modified}': {e}"
                    )

            # Verify checksum and size for MachineReadable files if not ignored
            if (
                original_checksum is not None
            ):  # Indicates it was a MachineReadable entry
                if ignore_checksum:
                    logger.debug(
                        f"Skipping checksum verification for '{current_output_path}' (--ignore-checksum flag set)."
                    )
                else:
                    try:
                        extracted_content_bytes = file_content_to_write.encode("utf-8")
                        calculated_checksum = hashlib.sha256(
                            extracted_content_bytes
                        ).hexdigest()

                        if calculated_checksum != original_checksum:
                            logger.warning(
                                f"CHECKSUM MISMATCH for file '{current_output_path}'. "
                                f"Expected: {original_checksum}, Calculated: {calculated_checksum}. "
                                f"The file content may be corrupted or was altered."
                            )
                        else:
                            logger.debug(
                                f"Checksum VERIFIED for file '{current_output_path}'."
                            )

                        if original_size_bytes is not None:
                            extracted_size_bytes = len(extracted_content_bytes)
                            if extracted_size_bytes != original_size_bytes:
                                logger.warning(
                                    f"SIZE MISMATCH for file '{current_output_path}'. "
                                    f"Expected: {original_size_bytes} bytes, Actual: {extracted_size_bytes} bytes."
                                )
                            else:
                                logger.debug(
                                    f"Size VERIFIED for file '{current_output_path}': {extracted_size_bytes} bytes."
                                )
                    except Exception as e:
                        logger.warning(
                            f"Error during checksum/size verification for '{current_output_path}': {e}"
                        )

        except Exception as e:
            logger.error(f"Failed to write file '{current_output_path}': {e}")
            files_failed_count += 1

    return files_created_count, files_overwritten_count, files_failed_count


def main():
    """
    Parses command-line arguments and orchestrates the file splitting process.
    It reads the combined input file, parses it to extract individual file data,
    and then writes these files to the specified destination directory.

    Character encoding control is provided in two ways:
    1. Using --respect-encoding to try to use the original encoding if available in metadata
    2. Using --target-encoding to explicitly specify an encoding for all output files
       (this overrides --respect-encoding if both are provided)

    By default, all files are written using UTF-8 encoding.
    """
    parser = argparse.ArgumentParser(
        description="Splits a combined file (from m1f.py) back into individual files.",
        epilog="Example: %(prog)s -i combined.txt -d ./output_src --force",
        formatter_class=argparse.RawTextHelpFormatter,
    )
    parser.add_argument(
        "-i",
        "--input-file",
        type=str,
        required=True,
        help="Path to the combined input file.",
    )
    parser.add_argument(
        "-d",
        "--destination-directory",
        type=str,
        required=True,
        help="Directory where extracted files will be saved.",
    )
    parser.add_argument(
        "-f",
        "--force",
        action="store_true",
        help="Force overwrite of existing files in the destination directory without prompting.",
    )
    parser.add_argument(
        "--verbose",
        "-v",
        action="store_true",
        help="Enable verbose output (DEBUG level logging).",
    )
    parser.add_argument(
        "--timestamp-mode",
        type=str,
        choices=["original", "current"],
        default="original",
        help="Specify how to set file timestamps:\\n"
        "  original: Try to restore original modification timestamp (default, only for MachineReadable format with timestamp).\\n"
        "  current: Use the current system timestamp for all extracted files.",
    )
    parser.add_argument(
        "--ignore-checksum",
        action="store_true",
        help="Skip checksum verification for MachineReadable files. Use this if the files were intentionally modified.",
    )
    parser.add_argument(
        "--respect-encoding",
        action="store_true",
        help="Try to use the original file encoding when writing extracted files. "
        "If enabled and original encoding is available, files will be written using "
        "that encoding instead of UTF-8. Falls back to UTF-8 for unsupported encodings.",
    )
    parser.add_argument(
        "--target-encoding",
        type=str,
        help="Explicitly specify the character encoding to use for all extracted files. "
        "This overrides the --respect-encoding option and any encoding information in the metadata. "
        "Examples: utf-8, utf-16-le, latin-1, cp1252, etc.",
    )

    args = parser.parse_args()
    _configure_logging(args.verbose)

    input_file_path = _resolve_input_file(args.input_file)
    dest_dir_path = _prepare_destination_dir(args.destination_directory)

    logger.info(f"Reading input file: {input_file_path}")
    try:
        combined_content = input_file_path.read_text(encoding="utf-8")
    except Exception as e:
        logger.error(f"Failed to read input file '{input_file_path}': {e}")
        sys.exit(1)

    logger.info("Parsing combined file content...")
    extracted_files_data = parse_combined_file(combined_content)

    if not extracted_files_data:
        logger.info("No files extracted. Output directory will be empty or unchanged.")
        sys.exit(0)

    files_created_count, files_overwritten_count, files_failed_count = (
        _write_extracted_files(
            dest_dir_path,
            extracted_files_data,
            args.force,
            args.timestamp_mode,
            args.ignore_checksum,
            args.respect_encoding,
            args.target_encoding,
        )
    )

    logger.info("File splitting process completed.")
    logger.info(f"Files created: {files_created_count}")
    logger.info(
        f"Files overwritten (with --force or confirmation): {files_overwritten_count}"
    )
    if files_failed_count > 0:
        logger.warning(f"Files failed to write: {files_failed_count}")

    if files_failed_count > 0:
        sys.exit(1)
    sys.exit(0)


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        # Ensure newline after ^C
        print(
            f"{(LF if os.name != 'nt' else CRLF)}Operation cancelled by user.",
            file=sys.stderr,
        )
        sys.exit(130)  # Standard exit code for Ctrl+C
    except SystemExit as e:
        # Catch sys.exit() calls to ensure they propagate correctly
        sys.exit(e.code)
    except Exception as e:
        # Fallback for any unexpected errors not caught in main()
        logger.critical(f"An unexpected critical error occurred: {e}", exc_info=True)
        sys.exit(1)
