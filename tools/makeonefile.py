#!/usr/bin/env python3
"""
===========================
PyMakeOneFile - File Combiner
===========================

SYNOPSIS
========
Combines the content of multiple text files from a specified directory and its
subdirectories into a single output file. Each file's content is preceded by
a separator showing metadata such as the file path, modification date, size,
and type.

DESCRIPTION
===========
This script is a Python alternative to the PowerShell `makeonefile.ps1` script.
It's designed to help consolidate multiple source code files or text documents
into a single file, which can be useful for:

- Code reviews (though dedicated review tools are often better for large projects).
- Creating a single document bundle for sharing or archiving.
- Simple documentation generation by concatenating markdown files.
- Creating a machine-parseable bundle for later splitting (using the 'MachineReadable' style).

KEY FEATURES
============
- Recursive file scanning in a source directory.
- Customizable separators between file contents ('Standard', 'Detailed', 'Markdown', 'MachineReadable').
- The 'MachineReadable' style uses unique boundary markers and JSON metadata for robust parsing.
- Option to add a timestamp to the output filename.
- Exclusion of common project directories (e.g., 'node_modules', '.git', 'build').
- Exclusion of binary files by default (based on extension).
- Option to include dot-files (e.g., '.gitignore') and binary files.
- Case-insensitive exclusion of additional specified directory names.
- Control over line endings (LF or CRLF) for script-generated separators.
- Verbose mode for detailed logging.
- Prompts for overwriting an existing output file unless `--force` is used.
- Estimates and displays the token count of the combined output file using tiktoken.

REQUIREMENTS
============
- Python 3.7+ (due to `pathlib` usage, f-strings, and json module usage)
- Standard Python libraries only (argparse, datetime, json, logging, os, pathlib, shutil, sys).
- tiktoken (for token counting of the output file)

INSTALLATION
============
No special installation is needed. Just download the script (`pymakeonefile.py`)
and ensure it has execute permissions if you want to run it directly
(e.g., `chmod +x pymakeonefile.py` on Linux/macOS).

USAGE
=====
Basic command:
  python pymakeonefile.py --source-directory /path/to/your/code --output-file /path/to/combined_output.txt

Using MachineReadable style for reliable splitting:
  python pymakeonefile.py -s ./my_project -o ./output/bundle.m1f --separator-style MachineReadable

With more options:
  python pymakeonefile.py -s ./my_project -o ./output/bundle.md -t --separator-style Markdown --force --verbose --additional-excludes "temp" "docs_old"

For all options, run:
  python pymakeonefile.py --help

NOTES
=====
- MachineReadable Format: This format is designed for automated splitting.
  It uses `>>>>>PYMAKEONEFILE_START_FILE<<<<<` and `>>>>>PYMAKEONEFILE_END_FILE<<<<<`
  as boundary markers, with a JSON object containing `{"path": "relative/path.ext"}`
  on the line immediately following the start marker.
- Binary Files: While the script can attempt to include binary files using the
  `--include-binary-files` flag, the content will be read as text (UTF-8 with
  error ignoring). This can result in garbled/unreadable content in the output
  and significantly increase file size. This feature is generally for including
  files that might be misidentified as binary or for specific edge cases.
- Performance: For extremely large directories with tens of thousands of files or
  very large individual files, the script might take some time to process.
- Encoding: The script attempts to read files as UTF-8 and writes the output file
  as UTF-8. Files with other encodings might not be handled perfectly, especially
  if they contain characters not representable in UTF-8 or if `errors='ignore'`
  has to discard characters.
- Line Endings of Source Files: The script preserves the original line endings of
  the content from the source files. The `--line-ending` option only affects the
  separators and blank lines *generated by this script*.

AUTHOR
======
Franz und Franz (Original PowerShell script)
AI (Python conversion and enhancements)

VERSION
=======
1.3.0 (Python Version, added MachineReadable format, added token counting)
"""

import argparse
import datetime
import hashlib
import json
import logging
import os
import sys
from pathlib import Path
from typing import List, Set, Tuple, Optional
import tiktoken  # Added for token counting

# --- Logger Setup ---
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO, format="%(levelname)-8s: %(message)s")

# --- Global Definitions ---

DEFAULT_EXCLUDED_DIR_NAMES = [
    "vendor",
    "node_modules",
    "build",
    "dist",
    "cache",
    ".git",  # Explicitly add common VCS directories
    ".svn",
    ".hg",
    "__pycache__",
]

BINARY_FILE_EXTENSIONS = {
    # Images
    ".jpg",
    ".jpeg",
    ".png",
    ".gif",
    ".bmp",
    ".tiff",
    ".tif",
    ".ico",
    ".webp",
    ".svgz",
    # Audio
    ".mp3",
    ".wav",
    ".ogg",
    ".flac",
    ".aac",
    ".wma",
    ".m4a",
    # Video
    ".mp4",
    ".avi",
    ".mkv",
    ".mov",
    ".wmv",
    ".flv",
    ".webm",
    ".mpeg",
    ".mpg",
    # Executables and compiled code
    ".exe",
    ".dll",
    ".so",
    ".dylib",
    ".bin",
    ".msi",
    ".pdb",
    ".lib",
    ".o",
    ".obj",
    ".pyc",
    ".pyo",
    ".class",
    # Archives
    ".zip",
    ".rar",
    ".7z",
    ".tar",
    ".gz",
    ".bz2",
    ".xz",
    ".jar",
    ".war",
    ".ear",
    ".cab",
    # Binary documents
    ".pdf",
    ".doc",
    ".ppt",  # Note: .docx, .xlsx, .pptx are XML-based (zip archives)
    # Databases & Data Files
    ".db",
    ".sqlite",
    ".mdb",
    ".accdb",
    ".dbf",
    ".dat",  # .dat is generic
    # Fonts
    ".ttf",
    ".otf",
    ".woff",
    ".woff2",
    ".eot",
    # Proprietary design formats
    ".psd",
    ".ai",
    ".indd",
    ".xd",
    ".fig",
    # Virtualization & Disk Images
    ".iso",
    ".img",
    ".vhd",
    ".vhdx",
    ".vmdk",
    # Other common binary types
    ".bak",
    ".tmp",
    ".lock",
    ".swo",
    ".swp",
}

# Line Ending Constants
LF = "\n"
CRLF = "\r\n"


# --- Token Counting Function ---
def _count_tokens_in_file_content(
    file_path_str: str, encoding_name: str = "cl100k_base"
) -> int:
    """
    Reads a file and counts the number of tokens using a specified tiktoken encoding.

    Args:
        file_path_str (str): The path to the file.
        encoding_name (str): The name of the encoding to use (e.g., "cl100k_base", "p50k_base").
                             "cl100k_base" is the encoding used by gpt-4, gpt-3.5-turbo.

    Returns:
        int: The number of tokens in the file.

    Raises:
        FileNotFoundError: If the specified file does not exist.
        Exception: For other issues like encoding errors or tiktoken issues.
    """
    if not os.path.exists(file_path_str):
        # This check is somewhat redundant if called after file creation,
        # but good for a generic helper.
        raise FileNotFoundError(f"Error: File not found at {file_path_str}")

    try:
        with open(file_path_str, "r", encoding="utf-8") as f:
            text_content = f.read()
    except UnicodeDecodeError:
        logger.debug(
            f"UTF-8 decoding failed for {file_path_str}, trying with error replacement."
        )
        with open(file_path_str, "rb") as f:
            byte_content = f.read()
        text_content = byte_content.decode("utf-8", errors="replace")
    except Exception as e:
        raise Exception(f"Error reading file {file_path_str} for token counting: {e}")

    try:
        encoding = tiktoken.get_encoding(encoding_name)
        tokens = encoding.encode(text_content)
        return len(tokens)
    except Exception as e:
        raise Exception(
            f"Error using tiktoken: {e}. Ensure tiktoken is installed and encoding_name is valid."
        )


# --- Helper Functions ---


def get_file_size_formatted(size_in_bytes: int) -> str:
    """Formats file size into a human-readable string (Bytes, KB, MB)."""
    if size_in_bytes < 1024:
        return f"{size_in_bytes} Bytes"
    elif size_in_bytes < (1024 * 1024):  # Less than 1 MB
        return f"{size_in_bytes / 1024:.2f} KB"
    else:  # 1 MB or more
        return f"{size_in_bytes / (1024 * 1024):.2f} MB"


def get_file_separator(
    file_info: Path, relative_path: str, style: str, linesep: str
) -> str:
    """
    Generates the file separator string based on the chosen style.

    Args:
        file_info: Path object for the file.
        relative_path: Relative path string of the file from the source directory.
        style: The separator style ('Standard', 'Detailed', 'Markdown', 'MachineReadable').
        linesep: The line separator string (LF or CRLF) to use.

    Returns:
        The formatted separator string.
    """
    stat_info = file_info.stat()
    mod_time = datetime.datetime.fromtimestamp(stat_info.st_mtime)
    mod_date_str = mod_time.strftime("%Y-%m-%d %H:%M:%S")
    file_size_bytes = stat_info.st_size
    file_size_hr = get_file_size_formatted(file_size_bytes)
    file_ext = file_info.suffix.lower() if file_info.suffix else "[no extension]"

    # Calculate checksum for all styles that will use it in the header
    checksum_sha256 = ""
    if style in ["Standard", "Detailed", "Markdown", "MachineReadable"]:
        try:
            # Read content once for checksum. This content is also written later.
            content_for_checksum = file_info.read_text(
                encoding="utf-8", errors="ignore"
            )
            checksum_sha256 = hashlib.sha256(
                content_for_checksum.encode("utf-8")
            ).hexdigest()
        except Exception as e:
            logger.warning(
                f"Could not read file {file_info} for checksum calculation: {e}. Checksum will be empty or not included."
            )
            checksum_sha256 = (
                "[CHECKSUM_ERROR]"  # Indicate error clearly if it was attempted
            )

    if style == "Standard":
        if checksum_sha256 and checksum_sha256 != "[CHECKSUM_ERROR]":
            return (
                f"======= {relative_path} | CHECKSUM_SHA256: {checksum_sha256} ======"
            )
        else:
            return f"======= {relative_path} ======"
    elif style == "Detailed":
        separator_lines = [
            "========================================================================================",
            f"== FILE: {relative_path}",
            f"== DATE: {mod_date_str} | SIZE: {file_size_hr} | TYPE: {file_ext}",
        ]
        if checksum_sha256 and checksum_sha256 != "[CHECKSUM_ERROR]":
            separator_lines.append(f"== CHECKSUM_SHA256: {checksum_sha256}")
        separator_lines.append(
            "========================================================================================"
        )
        return linesep.join(separator_lines)
    elif style == "Markdown":
        md_lang_hint = (
            file_info.suffix[1:]
            if file_info.suffix and len(file_info.suffix) > 1
            else ""
        )
        metadata_line = f"**Date Modified:** {mod_date_str} | **Size:** {file_size_hr} | **Type:** {file_ext}"
        if checksum_sha256 and checksum_sha256 != "[CHECKSUM_ERROR]":
            metadata_line += f" | **Checksum (SHA256):** {checksum_sha256}"
        separator_lines = [
            f"## {relative_path}",
            metadata_line,
            "",  # Empty line before code block
            f"```{md_lang_hint}",
        ]
        return linesep.join(separator_lines)
    elif style == "MachineReadable":
        # Checksum calculation moved above for all relevant styles
        meta = {
            "path": relative_path,
            "modified": mod_date_str,
            "type": file_ext,
            "size_bytes": file_size_bytes,
            "checksum_sha256": (
                checksum_sha256 if checksum_sha256 != "[CHECKSUM_ERROR]" else ""
            ),
        }
        json_meta = json.dumps(meta)
        return f">>>>>PYMAKEONEFILE_START_FILE<<<<<{linesep}" f"{json_meta}{linesep}"
    else:  # Should not happen due to argparse choices
        logger.warning(f"Unknown separator style '{style}'. Falling back to basic.")
        return f"--- {relative_path} ---"


def get_closing_separator(style: str, linesep: str) -> str | None:
    """
    Generates the closing separator string, if any.

    Args:
        style: The separator style.
        linesep: The line separator string (LF or CRLF) to use.

    Returns:
        The closing separator string, or None if no closing separator is needed.
    """
    if style == "Markdown":
        return "```"
    elif style == "MachineReadable":
        # The preceding linesep ensures the marker is on its own line after content.
        return f"{linesep}>>>>>PYMAKEONEFILE_END_FILE<<<<<"
    return None


# --- Refactored Helper Functions for main() ---


def _configure_logging_settings(verbose: bool, chosen_linesep: str) -> None:
    """Configures logging level based on verbosity."""
    if verbose:
        logging.getLogger().setLevel(logging.DEBUG)
        logger.debug("Verbose mode enabled.")
        logger.debug(f"Using line ending: {'LF' if chosen_linesep == LF else 'CRLF'}")


def _resolve_and_validate_source_path(source_directory_str: str) -> Path:
    """Resolves and validates the source directory path."""
    try:
        source_dir = Path(source_directory_str).resolve(strict=True)
        if not source_dir.is_dir():
            logger.error(f"Source path '{source_dir}' is not a directory.")
            sys.exit(1)
        return source_dir
    except FileNotFoundError:
        logger.error(f"Source directory '{source_directory_str}' not found.")
        sys.exit(1)
    except Exception as e:
        logger.error(f"Error resolving source directory '{source_directory_str}': {e}")
        sys.exit(1)


def _prepare_output_file_path(output_file_str: str, add_timestamp: bool) -> Path:
    """Prepares the output file path, including timestamp if requested."""
    output_file_path = Path(output_file_str).resolve()
    if add_timestamp:
        timestamp = datetime.datetime.now().strftime("_%Y%m%d_%H%M%S")
        output_file_path = output_file_path.with_name(
            f"{output_file_path.stem}{timestamp}{output_file_path.suffix}"
        )
        logger.debug(f"Output filename with timestamp: {output_file_path.name}")
    return output_file_path


def _handle_output_file_overwrite_and_creation(
    output_file_path: Path, force: bool, chosen_linesep: str
) -> None:
    """Handles output file existence, overwrite confirmation, and parent directory creation."""
    if output_file_path.exists() and output_file_path.is_file():
        if force:
            logger.warning(
                f"Output file '{output_file_path}' exists. Overwriting due to --force."
            )
            try:
                output_file_path.unlink()
            except Exception as e:
                logger.error(
                    f"Could not remove existing output file '{output_file_path}': {e}"
                )
                sys.exit(1)
        else:
            try:
                confirmation = input(
                    f"Output file '{output_file_path}' already exists. Overwrite? (y/N): "
                )
                if confirmation.lower() != "y":
                    logger.info("Operation cancelled by user.")
                    sys.exit(0)
                output_file_path.unlink()
                logger.debug(f"Overwriting existing output file '{output_file_path}'.")
            except KeyboardInterrupt:
                logger.info(f"{chosen_linesep}Operation cancelled by user (Ctrl+C).")
                sys.exit(0)
            except Exception as e:
                logger.error(
                    f"Could not remove existing output file '{output_file_path}': {e}"
                )
                sys.exit(1)
    elif output_file_path.exists() and not output_file_path.is_file():
        logger.error(
            f"Output path '{output_file_path}' exists but is not a file (e.g., it's a directory)."
        )
        sys.exit(1)

    try:
        output_file_path.parent.mkdir(parents=True, exist_ok=True)
    except Exception as e:
        logger.error(
            f"Could not create output directory '{output_file_path.parent}': {e}"
        )
        sys.exit(1)


def _build_exclusion_set(additional_excludes: list[str]) -> set[str]:
    """Builds the set of lowercased directory names to exclude."""
    all_excluded_dir_names_lower = {name.lower() for name in DEFAULT_EXCLUDED_DIR_NAMES}
    all_excluded_dir_names_lower.update(name.lower() for name in additional_excludes)
    logger.debug(
        f"Effective excluded directory names (case-insensitive): {sorted(list(all_excluded_dir_names_lower))}"
    )
    return all_excluded_dir_names_lower


def _deduplicate_paths(path_objects: List[Path]) -> List[Path]:
    """
    Deduplicate paths by removing any paths that are children of other paths in the list.

    Args:
        path_objects: List of Path objects to deduplicate

    Returns:
        List of Path objects with no child paths if their parent is already in the list
    """
    if not path_objects:
        return []

    # Sort by path length (shortest first) to ensure we process parent directories first
    path_objects.sort(key=lambda p: len(p.parts))

    # Keep track of paths to include (initially all)
    include_paths = set(path_objects)

    # Check each path to see if it's a child of any other path
    for i, path in enumerate(path_objects):
        # Skip if already excluded
        if path not in include_paths:
            continue

        # Check against all other paths
        for other_path in path_objects[i + 1 :]:
            # If this path is a parent of another path, exclude the child
            try:
                # Use str path comparison for Python 3.7+ compatibility (instead of is_relative_to from 3.9+)
                other_path_str = str(other_path.absolute())
                path_str = str(path.absolute())

                # Check if other_path is a subpath of path (e.g., path="/a/b", other_path="/a/b/c")
                # Ensure it's not just a common prefix (e.g., path="/a/b", other_path="/a/b_ext")
                if (
                    len(other_path_str) > len(path_str)
                    and other_path_str.startswith(path_str)
                    and other_path_str[len(path_str)] == os.sep
                ):
                    include_paths.discard(other_path)
            except (ValueError, RuntimeError):
                # Handle potential path resolution issues
                continue

    return sorted(include_paths)


def _process_paths_from_input_file(input_file_path: Path) -> List[Path]:
    """
    Process a file containing paths (one per line) and return a list of Path objects.

    Args:
        input_file_path: Path to the input file containing paths to process

    Returns:
        List of deduplicated paths with proper parent-child handling
    """
    paths = []

    try:
        with open(input_file_path, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line or line.startswith("#"):
                    continue  # Skip empty lines and comments

                # Convert to absolute path if not already
                path = Path(line).expanduser().resolve()
                paths.append(path)

        # Deduplicate paths (keep parents, remove children)
        return _deduplicate_paths(paths)
    except Exception as e:
        logger.error(f"Error processing input file: {e}")
        return []


def _is_file_excluded(
    file_path: Path, args: argparse.Namespace, all_excluded_dir_names_lower: Set[str]
) -> bool:
    """Checks if a file should be excluded based on various criteria."""
    if not args.include_dot_files and file_path.name.startswith("."):
        if args.verbose:
            logger.debug(f"Excluding dot file: {file_path}")
        return True

    if (
        not args.include_binary_files
        and file_path.suffix.lower() in BINARY_FILE_EXTENSIONS
    ):
        if args.verbose:
            logger.debug(
                f"Excluding binary file: {file_path} (extension: {file_path.suffix.lower()})"
            )
        return True

    # Check if any parent directory is in the global exclude list
    # This iterates from file_path.parent up to the root.
    p = file_path.parent
    while p != p.parent:  # Stop when p is the root (e.g. '/' or 'C:\')
        if p.name and p.name.lower() in all_excluded_dir_names_lower:
            if args.verbose:
                logger.debug(
                    f"Excluding file '{file_path}' because parent directory '{p.name}' (path: {p}) is in exclude list."
                )
            return True
        # p.parent of root is root itself, so this condition handles termination.
        if p == p.parent:
            break
        p = p.parent
    return False


def _gather_files_to_process(
    source_dir: Path,
    args: argparse.Namespace,
    all_excluded_dir_names_lower: Set[str],
    input_paths: Optional[List[Path]] = None,
) -> List[Tuple[Path, str]]:
    """
    Gather files to process, either from a source directory or from a list of input paths.

    Args:
        source_dir: The source directory (used when not using input paths)
        args: Command line arguments
        all_excluded_dir_names_lower: Set of directory names to exclude (lowercase)
        input_paths: Optional list of paths from input file

    Returns:
        List of tuples containing (file_path, relative_path)
    """
    files_to_process = []
    # Use a set to track absolute paths of files already added to avoid duplicates
    # especially when using --input-file which might list overlapping paths or individual files within listed dirs.
    added_file_absolute_paths: Set[str] = set()

    if input_paths is not None:
        logger.info(
            f"Processing {len(input_paths)} unique path(s) from input file '{args.input_file}'..."
        )
        for (
            item_path
        ) in input_paths:  # Each item_path is an absolute, resolved, deduplicated path
            if (
                not item_path.exists()
            ):  # Should be rare due to prior checks but good for safety
                logger.warning(f"Path from input file no longer exists: {item_path}")
                continue

            if item_path.is_file():
                abs_path_str = str(item_path.resolve())
                if abs_path_str in added_file_absolute_paths:
                    if args.verbose:
                        logger.debug(f"Skipping already added file: {item_path}")
                    continue
                if not _is_file_excluded(item_path, args, all_excluded_dir_names_lower):
                    files_to_process.append((item_path, item_path.name))
                    added_file_absolute_paths.add(abs_path_str)
                # else: _is_file_excluded logs if verbose

            elif item_path.is_dir():
                # Check if the directory itself is in the excluded names list
                if item_path.name.lower() in all_excluded_dir_names_lower:
                    if args.verbose:
                        logger.debug(
                            f"Skipping directory '{item_path}' from input list as its name is excluded."
                        )
                    continue

                if args.verbose:
                    logger.debug(f"Scanning directory from input file: {item_path}")
                for file_path_in_dir in item_path.rglob("*"):
                    if file_path_in_dir.is_file():
                        abs_path_str = str(file_path_in_dir.resolve())
                        if abs_path_str in added_file_absolute_paths:
                            if args.verbose:
                                logger.debug(
                                    f"Skipping already added file: {file_path_in_dir}"
                                )
                            continue
                        if not _is_file_excluded(
                            file_path_in_dir, args, all_excluded_dir_names_lower
                        ):
                            relative_path = file_path_in_dir.relative_to(item_path)
                            files_to_process.append(
                                (file_path_in_dir, str(relative_path))
                            )
                            added_file_absolute_paths.add(abs_path_str)
                        # else: _is_file_excluded logs if verbose
    else:
        # Original directory walking logic
        logger.info(f"Scanning files in '{source_dir}'...")
        if args.verbose:
            logger.debug(
                f"  Exclusion settings: include_dot_files={args.include_dot_files}, "
                f"include_binary_files={args.include_binary_files}"
            )
            logger.debug(
                f"  Excluded directory names (case-insensitive): {sorted(list(all_excluded_dir_names_lower))}"
            )

        for file_path in source_dir.rglob("*"):
            if not file_path.is_file():
                continue  # Skip directories

            # No need to check added_file_absolute_paths here because rglob from a single source_dir should not yield duplicates by its nature.
            if not _is_file_excluded(file_path, args, all_excluded_dir_names_lower):
                relative_path = file_path.relative_to(source_dir)
                files_to_process.append((file_path, str(relative_path)))
            # else: _is_file_excluded logs if verbose

    return sorted(files_to_process, key=lambda x: str(x[1]).lower())


def _write_combined_data(
    output_file_path: Path,
    files_to_process: list[tuple[Path, str]],
    args: argparse.Namespace,
    chosen_linesep: str,
) -> int:
    """Writes the combined file data to the output file and returns the count of processed files."""
    total_files = len(files_to_process)
    logger.info(f"Processing {total_files} file(s) for inclusion...")
    file_counter = 0
    try:
        with open(output_file_path, "w", encoding="utf-8") as outfile:
            for file_info, rel_path_str in files_to_process:
                file_counter += 1
                logger.debug(
                    f"Processing file ({file_counter}/{total_files}): {file_info.name} (Rel: {rel_path_str})"
                )

                separator_text = get_file_separator(
                    file_info, rel_path_str, args.separator_style, chosen_linesep
                )

                # For MachineReadable, separator_text already includes its final necessary newline after JSON.
                # For other styles, we add one after the separator text itself.
                if args.separator_style == "MachineReadable":
                    outfile.write(separator_text)
                else:
                    outfile.write(separator_text + chosen_linesep)

                # Add an additional blank line for Standard and Detailed styles
                # after their header and before the actual file content begins.
                if args.separator_style in ["Standard", "Detailed"]:
                    outfile.write(chosen_linesep)

                content = ""  # Initialize content to ensure it's defined
                try:
                    content = file_info.read_text(encoding="utf-8", errors="ignore")
                    outfile.write(content)
                except Exception as e:
                    error_message = (
                        f"[ERROR: UNABLE TO READ FILE '{file_info}'. REASON: {e}]"
                    )
                    logger.warning(error_message)
                    outfile.write(error_message + chosen_linesep)

                # Ensure content block is followed by a newline if it didn't originally have one,
                # for styles where the closing separator doesn't already handle this.
                if (
                    args.separator_style != "MachineReadable"
                    and content
                    and not content.endswith(("\n", "\r"))
                ):
                    outfile.write(chosen_linesep)
                # No special handling needed here if content is empty, as the subsequent
                # closing_separator and inter-file newlines will still be added correctly.

                closing_separator_text = get_closing_separator(
                    args.separator_style, chosen_linesep
                )
                if closing_separator_text:
                    # For MachineReadable, closing_separator_text is `linesep + END_MARKER`.
                    #   outfile.write adds `linesep + END_MARKER + chosen_linesep`.
                    # For Markdown, closing_separator_text is ```` ``` ``` `.
                    #   outfile.write adds ```` ``` ``` ` + chosen_linesep`.
                    outfile.write(closing_separator_text + chosen_linesep)

                # Add a separating newline IF this is not the last file.
                # This serves as the blank line between file entries for all styles.
                if file_counter < total_files:
                    outfile.write(chosen_linesep)
        return file_counter
    except IOError as e:
        logger.error(f"An I/O error occurred writing to '{output_file_path}': {e}")
        sys.exit(1)
    except Exception as e:
        logger.error(
            f"An unexpected error occurred during file processing: {e}",
            exc_info=args.verbose,
        )
        sys.exit(1)


# --- Main Script Logic ---


def main():
    """
    Main function to parse arguments and orchestrate the file combination process.
    """
    parser = argparse.ArgumentParser(
        description="Combines the content of multiple text files into a single output file, with metadata. \n"
        "Useful for code reviews, documentation bundling, or sharing multiple files as one.",
        epilog="""Examples:
  %(prog)s --source-directory "./src" --output-file "combined_files.txt"
  %(prog)s -s "/home/user/projects/my_app" -o "/tmp/app_bundle.md" -t --separator-style Markdown --force
  %(prog)s -s . -o archive.txt --additional-excludes "docs_archive" "test_data" --include-dot-files --line-ending crlf
  %(prog)s -s ./config_files --include-dot-files --include-binary-files -o all_configs.txt --verbose""",
        formatter_class=argparse.RawTextHelpFormatter,
    )

    # Input source options are defined in a mutually exclusive group below
    parser.add_argument(
        "-o",
        "--output-file",
        type=str,
        required=True,
        help="Path where the combined output file will be created.",
    )
    parser.add_argument(
        "-f",
        "--force",
        action="store_true",
        help="Force overwrite of output file without prompting.",
    )
    parser.add_argument(
        "-t",
        "--add-timestamp",
        action="store_true",
        help="Add a timestamp (_yyyyMMdd_HHmmss) to the output filename.",
    )
    parser.add_argument(
        "--additional-excludes",
        type=str,
        nargs="*",
        default=[],
        metavar="DIR_NAME",
        help="Space-separated list of additional directory NAMES to exclude (e.g., 'obj', 'debug'). Case-insensitive.",
    )
    parser.add_argument(
        "--include-dot-files",
        action="store_true",
        help="Include files that start with a dot (e.g., .gitignore). Dot directories (e.g. .git) are generally excluded by default_excluded_dir_names.",
    )
    parser.add_argument(
        "--include-binary-files",
        action="store_true",
        help="Attempt to include files with binary extensions. Content may be unreadable. Use with caution.",
    )
    parser.add_argument(
        "--separator-style",
        choices=["Standard", "Detailed", "Markdown", "MachineReadable"],
        default="Detailed",
        help="Format of the separator between files. \n"
        "  'Standard': Simple path display.\n"
        "  'Detailed': Path, date, size, type (default).\n"
        "  'Markdown': Markdown H2 for path, metadata, and code block.\n"
        "  'MachineReadable': Uses unique boundary markers and JSON for robust splitting.",
    )
    parser.add_argument(
        "--line-ending",
        choices=["lf", "crlf"],
        default="lf",
        help="Line ending for script-generated separators/newlines. 'lf' (Unix) or 'crlf' (Windows). Default: lf. Does not change line endings of original file content.",
    )
    parser.add_argument(
        "--verbose",
        "-v",
        action="store_true",
        help="Enable verbose output (DEBUG level logging).",
    )

    input_group = parser.add_mutually_exclusive_group(required=True)
    input_group.add_argument(
        "-s",
        "--source-directory",
        type=str,
        help="Path to the directory containing the files to be combined.",
    )
    input_group.add_argument(
        "-i",
        "--input-file",
        type=str,
        help="Path to a text file containing a list of files and directories to process (one per line).",
    )

    args = parser.parse_args()

    chosen_linesep = LF if args.line_ending == "lf" else CRLF
    _configure_logging_settings(args.verbose, chosen_linesep)

    # Process input file if provided, otherwise use source directory
    input_paths = None
    if hasattr(args, "input_file") and args.input_file:
        input_file_path = Path(args.input_file).resolve()
        if not input_file_path.exists() or not input_file_path.is_file():
            logger.error(f"Input file not found: {input_file_path}")
            sys.exit(1)
        input_paths = _process_paths_from_input_file(input_file_path)
        logger.info(f"Found {len(input_paths)} paths to process from input file")

        # Use the first path's parent as the base directory for relative paths
        source_dir = input_paths[0].parent if input_paths else Path.cwd()
    else:
        source_dir = _resolve_and_validate_source_path(args.source_directory)

    output_file_path = _prepare_output_file_path(args.output_file, args.add_timestamp)
    _handle_output_file_overwrite_and_creation(
        output_file_path, args.force, chosen_linesep
    )

    all_excluded_dir_names_lower = _build_exclusion_set(args.additional_excludes)
    files_to_process = _gather_files_to_process(
        source_dir, args, all_excluded_dir_names_lower, input_paths
    )

    if not files_to_process:
        logger.warning(f"No files found matching the criteria in '{source_dir}'.")
        try:
            with open(output_file_path, "w", encoding="utf-8") as f:
                f.write(f"# No files processed from {source_dir}{chosen_linesep}")
            logger.info(
                f"Empty output file (with a note) created at '{output_file_path}'."
            )
        except Exception as e:
            logger.error(
                f"Could not create empty output file '{output_file_path}': {e}"
            )
            sys.exit(1)
        sys.exit(0)

    processed_count = _write_combined_data(
        output_file_path, files_to_process, args, chosen_linesep
    )

    logger.info(
        f"Successfully combined {processed_count} file(s) into '{output_file_path}'."
    )

    # --- Token Counting for Output File ---
    if processed_count > 0:  # Only count tokens if files were actually processed
        try:
            logger.info(f"Attempting to count tokens in '{output_file_path.name}'...")
            # Defaulting to "cl100k_base" as it's common for GPT models
            token_count = _count_tokens_in_file_content(str(output_file_path))
            logger.info(
                f"The output file '{output_file_path.name}' contains approximately {token_count} tokens (using 'cl100k_base' encoding)."
            )
        except FileNotFoundError:
            # Should ideally not happen if _write_combined_data was successful
            logger.warning(
                f"Could not count tokens: Output file '{output_file_path.name}' not found after creation."
            )
        except Exception as e:
            logger.warning(f"Could not count tokens for '{output_file_path.name}': {e}")
            logger.warning(
                "  To enable token counting, please ensure 'tiktoken' library is installed (e.g., 'pip install tiktoken')."
            )
    elif output_file_path.exists():  # An empty note file might have been created
        logger.info(
            f"Output file '{output_file_path.name}' was created but is empty or contains no processed files; skipping token count."
        )

    sys.exit(0)


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        # Print newline so the prompt isn't on the same line as ^C
        print(f"{LF}Operation cancelled by user.", file=sys.stderr)
        sys.exit(130)  # Standard exit code for Ctrl+C
    except SystemExit as e:
        # Catch sys.exit() calls to ensure they propagate correctly
        sys.exit(e.code)
    except Exception as e:
        # Fallback for any unexpected errors not caught in main()
        logger.critical(f"An unexpected critical error occurred: {e}", exc_info=True)
        sys.exit(1)
